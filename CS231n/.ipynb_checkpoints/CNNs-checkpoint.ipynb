{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "For 1D, or 2D (or more??) where data is orgainsed linearly. Local relationships/patterns! (RNNs are non-local???)\n",
    "\n",
    "### Motivation\n",
    "\n",
    "By convolving a single neuron (with the same weights) over an entire image we reduce the number of parameters required (by how much ???). This makes it easier to for the net to learn as there are less parameters to train, (and somehow bounds the complexity ??)\n",
    "\n",
    "### Convolution integral\n",
    "What does convolution mean? And how does it apply here?? And how is it implemented (efficiently??\n",
    "\n",
    "### Filters\n",
    "\n",
    "Why size x? Why, x structure?\n",
    "\n",
    "Do the filters always have to be square? What about different filter shapes for different data structures/encodings (polar, tree, ??)? How could this help us recognise certain patterns? Or, does if not matter as the CNN a universal recogniser (could we prove that any/all features can be represented)? Although, it could make it faster?\n",
    "\n",
    "### Padding\n",
    "* Zero border padding. Makes sense as otherwise it could be hard to learn to classify. The border cells will only appear once\n",
    "* What if you randomly zero pad inputs? Does this effect the network similar to dropout?\n",
    "\n",
    "### Residual nets\n",
    "\n",
    "- carrying old info forward\n",
    "\n",
    "### Training\n",
    "\n",
    "How did they actually train the LeNET or the AlexNet?\n",
    "I still dont understand how the depth of the nets can be trained\n",
    "Dropout?\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Guided backprop\n",
    "\n",
    "Deconvolving???\n",
    "\n",
    "\n",
    "\n",
    "Transfer learning - so it already has a basic set of features. Edge detectors, words, faces, ... ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The different layer types\n",
    "class Layer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x, gradient = 1):\n",
    "        x[x<0] = 0\n",
    "        x = x * gradient\n",
    "        return x\n",
    "    \n",
    "class Filter(Layer):\n",
    "    def __init__(self, shape):\n",
    "        self.weights = np.zeros(shape)\n",
    "        \n",
    "    def forward(self, x, stride = 3, padding = 0, window = 9): #convolve\n",
    "        x = pad(x,padding)\n",
    "        n,m,l = x.shape\n",
    "        for i in range((n-window)/stride + 1):\n",
    "            for j in range((m-window)/stride + 1):\n",
    "                y[i][j] = np.dot(self.weights,x[i,j]) + bias #nxmxl x \n",
    "        return y\n",
    "        \n",
    "### Putting it all together\n",
    "import Network as Net #contains nice formatting tools \n",
    "class ConvNN(Net):\n",
    "    def __init__(self):\n",
    "        self.layers\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
