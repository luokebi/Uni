{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could do this with css??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import tensorflow as tf # :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* [Definition](#Definition)\n",
    "* [Motivation](#Motivation)\n",
    "* [Background](#Background)\n",
    "* [Method](#Method)\n",
    "* [Implementation](#Implementation)\n",
    "* [Results](#Results)\n",
    "* [Questions](#Questions)\n",
    "* [References](#References)\n",
    "\n",
    "# Definition <a id='Definition'></a>\n",
    "\n",
    "For 1D, or 2D (or more??) where data is orgainsed linearly. Local relationships/patterns! (RNNs are non-local???)\n",
    "\n",
    "# Motivation <a id='Motivation'></a>\n",
    "\n",
    "\n",
    "By convolving a single neuron (with the same weights) over an entire image we reduce the number of parameters required (by how much ???). This makes it easier to for the net to learn as there are less parameters to train, (and somehow bounds the complexity ??)\n",
    "\n",
    "# Background <a id='Background'></a>\n",
    "\n",
    "\n",
    "## Convolution integral\n",
    "What does convolution mean? And how does it apply here?? And how is it implemented (efficiently??\n",
    "\n",
    "## Filters\n",
    "\n",
    "Why size x? Why, x structure?\n",
    "\n",
    "Do the filters always have to be square? What about different filter shapes for different data structures/encodings (polar, tree, ??)? How could this help us recognise certain patterns? Or, does if not matter as the CNN a universal recogniser (could we prove that any/all features can be represented)? Although, it could make it faster?\n",
    "\n",
    "## Rectified linear units\n",
    "\n",
    "Why are these better than other? (maybe this is not the right place to investigate this)\n",
    "\n",
    "## Max pooling\n",
    "\n",
    "Why max? Motivation??\n",
    "\n",
    "## Padding\n",
    "* Zero border padding. Makes sense as otherwise it could be hard to learn to classify. The border cells will only appear once\n",
    "* What if you randomly zero pad inputs? Does this effect the network similar to dropout?\n",
    "\n",
    "## Residual nets\n",
    "\n",
    "- carrying old info forward\n",
    "\n",
    "## Normalising\n",
    "\n",
    "\n",
    "### Weights\n",
    "\n",
    "\n",
    "### Batch normalisation\n",
    "\n",
    "\n",
    "Variance can be learned - (David Balduzzi)\n",
    "\n",
    "## Training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Changing the learning rates\n",
    "\n",
    "... \n",
    "## Dropout\n",
    "\n",
    "Dropout? (a nice side effect of this, or maybe intended effect) is that it would also make it easier to train? as derivatives can be backpropagated for larger distances without exploding or vanishing\n",
    "\n",
    "\n",
    "## Analysis\n",
    "\n",
    "Guided backprop\n",
    "\n",
    "Deconvolving???\n",
    "\n",
    "\n",
    "\n",
    "Transfer learning - so it already has a basic set of features. Edge detectors, words, faces, ... ?\n",
    "\n",
    "\n",
    "\n",
    "# Method <a id='Method'></a>\n",
    "\n",
    "\n",
    "## CIFAR 10\n",
    "\n",
    "32 x 32 = 1024 pixels. with 3 values in each, red, green, blue. = 3 * 1024 = 3072\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation <a id='Implementation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The different layer types\n",
    "class Layer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x, gradient = 1):\n",
    "        x[x<0] = 0\n",
    "        x = x * gradient\n",
    "        return x\n",
    "    \n",
    "class Filter(Layer):\n",
    "    def __init__(self, shape):\n",
    "        self.weights = np.zeros(shape)\n",
    "        \n",
    "    def forward(self, x, stride = 3, padding = 0, window = 9): #convolve\n",
    "        x = pad(x,padding)\n",
    "        n,m,l = x.shape\n",
    "        for i in range((n-window)/stride + 1):\n",
    "            for j in range((m-window)/stride + 1):\n",
    "                y[i][j] = np.dot(self.weights,x[i,j]) + bias #nxmxl x \n",
    "        return y\n",
    "        \n",
    "### Putting it all together\n",
    "import Network as Net #contains nice formatting tools \n",
    "class ConvNN(Net):\n",
    "    def __init__(self):\n",
    "        self.layers\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Results <a id='Results'></a>\n",
    "\n",
    " \n",
    "# Questions and thoughts<a id='Questions'></a>\n",
    "\n",
    "\n",
    "* The greater the depth the more the gradients vanish/explode. How do deep conv nets avoid this? Normalise the weights to be on average 1. Thus, between each layer $\\delta \\cdot w = \\delta \\cdot 1$. That applies to linear depth, but what about residual nets?\n",
    "* How does dropout work in conv nets? it is not like we have hidden layers were it is not a big deal if nodes disappear. The means between each layer you need to be able to ?? Dropping a node is the same as removing a column?\n",
    "* CNNs are attractive because of their efficiency. The weight tying means that less parameters need to be learned. This is why recurrent NNs are also capable of learning interesting things in/with reasonable amount of training data and computational steps.\n",
    " * This seems really important. How can it be taken further? \n",
    " * This also seems deeply linked with the use of convolution?\n",
    "\n",
    "\n",
    "# Resources and references <a id='References'></a>\n",
    "\n",
    "* [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)\n",
    "* [Tensor flow tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/deep_cnn/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-efd9885fd55f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-efd9885fd55f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "body { background-color:blue; }\n",
       ".highlightme { background-color:#FFFF00; }\n",
       "p { background-color:#FFFFFF; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def css_styling():\n",
    "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
